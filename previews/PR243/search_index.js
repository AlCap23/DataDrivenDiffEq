var documenterSearchIndex = {"docs":
[{"location":"utils/#Collocation","page":"Utilities","title":"Collocation","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"InterpolationMethod\ncollocate_data","category":"page"},{"location":"utils/#DataDrivenDiffEq.InterpolationMethod","page":"Utilities","title":"DataDrivenDiffEq.InterpolationMethod","text":"A wrapper for the interpolation methods of DataInterpolations.jl.\n\nWraps the methods in such a way that they are callable as f(u,t) to create and return an interpolation of u over t. The first argument of the constructor always defines the interpolation method, all following arguments will be used in the interpolation.\n\nExample\n\n# Create the wrapper struct\nitp_method = InterpolationMethod(QuadraticSpline)\n# Create a callable interpolation\nitp = itp_method(u,t)\n# Return u[2]\nitp(t[2])\n\n\n\n\n\n","category":"type"},{"location":"utils/#DataDrivenDiffEq.collocate_data","page":"Utilities","title":"DataDrivenDiffEq.collocate_data","text":"collocate_data(data, tpoints)\ncollocate_data(data, tpoints, kernel)\n\n\nUnified interface for collocation techniques. The input can either be a CollocationKernel (see list below) or a wrapped InterpolationMethod from DataInterpolations.jl.\n\nComputes a non-parametrically smoothed estimate of u' and u given the data, where each column is a snapshot of the timeseries at tpoints[i].\n\nExamples\n\nu′,u = collocate_data(data,tpoints,kernel=SigmoidKernel())\nu′,u = collocate_data(data,tpoints,tpoints_sample,interp,args...)\nu′,u = collocate_data(data,tpoints,interp)\n\nCollocation Kernels\n\nSee this paper for more information.\n\nEpanechnikovKernel\nUniformKernel\nTriangularKernel\nQuarticKernel\nTriweightKernel\nTricubeKernel\nGaussianKernel\nCosineKernel\nLogisticKernel\nSigmoidKernel\nSilvermanKernel\n\nInterpolation Methods\n\nSee DataInterpolations.jl for more information.\n\nConstantInterpolation\nLinearInterpolation\nQuadraticInterpolation\nLagrangeInterpolation\nQuadraticSpline\nCubicSpline\nBSplineInterpolation\nBSplineApprox\nCurvefit\n\n\n\n\n\n","category":"function"},{"location":"utils/#Utilities","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"utils/","page":"Utilities","title":"Utilities","text":"AIC\nAICC\nBIC\noptimal_shrinkage\nburst_sampling\nsubsample","category":"page"},{"location":"utils/#DataDrivenDiffEq.AIC","page":"Utilities","title":"DataDrivenDiffEq.AIC","text":"AIC(k, X, Y; likelihood)\n\n\nComputes the Akaike Information Criterion (AIC) given the free parameters k for the data X and its estimate Y of the model. likelihood can be any function of X and Y.\n\n\n\n\n\n","category":"function"},{"location":"utils/#DataDrivenDiffEq.AICC","page":"Utilities","title":"DataDrivenDiffEq.AICC","text":"AICC(k, X, Y; likelihood)\n\n\nComputes the Akaike Information Criterion compensated for finite samples (AICC) given the free parameters k for the data X and its estimate Y of the model. likelihood can be any function of X and Y.\n\n\n\n\n\n","category":"function"},{"location":"utils/#DataDrivenDiffEq.BIC","page":"Utilities","title":"DataDrivenDiffEq.BIC","text":"BIC(k, X, Y; likelihood)\n\n\nComputes Bayes Information Criterion (BIC) given the free parameters k for the data X and its estimate Y of the model. likelihood can be any function of X and Y.\n\n\n\n\n\n","category":"function"},{"location":"utils/#DataDrivenDiffEq.optimal_shrinkage","page":"Utilities","title":"DataDrivenDiffEq.optimal_shrinkage","text":"optimal_shrinkage(X)\n\n\nCompute a feature reduced version of the data array X via thresholding the singular values by computing the optimal threshold for singular values.\n\n\n\n\n\n","category":"function"},{"location":"utils/#DataDrivenDiffEq.burst_sampling","page":"Utilities","title":"DataDrivenDiffEq.burst_sampling","text":"(\n\nburst_sampling(x, samplesize, bursts)\n\n\n)\n\nRandomly selects n bursts of data with size samplesize from the data X.\n\nRandomly selects n bursts of data with size samplesize from the data X and Y.\n\nRandomly selects n bursts of data within a time window period from the data X. The time information has to be provided in t.\n\n\n\n\n\n","category":"function"},{"location":"utils/#DataDrivenDiffEq.subsample","page":"Utilities","title":"DataDrivenDiffEq.subsample","text":"subsample(x, frequency)\n\n\nReturns the subsampled X with only every n-th entry.\n\nReturns the subsampled X with a a minimum period of dt between two data points. t provides the time information.\n\n\n\n\n\n","category":"function"},{"location":"basis/#Basis","page":"Basis","title":"Basis","text":"","category":"section"},{"location":"basis/","page":"Basis","title":"Basis","text":"Basis","category":"page"},{"location":"basis/#DataDrivenDiffEq.Basis","page":"Basis","title":"DataDrivenDiffEq.Basis","text":"mutable struct Basis <: DataDrivenDiffEq.AbstractBasis\n\nA basis over the states with parameters , independent variable  and possible exogenous controls. It extends an AbstractSystem as defined in ModelingToolkit.jl. f can either be a Julia function which is able to use ModelingToolkit variables or a vector of eqs. It can be called with the typical SciML signature, meaning out of place with f(u,p,t) or in place with f(du, u, p, t). If control inputs are present, it is assumed that no control corresponds to zero for all inputs. The corresponding function calls are f(u,p,t,inputs) and f(du,u,p,t,inputs) and need to be specified fully.\n\nIf linear_independent is set to true, a linear independent basis is created from all atom function in f.\n\nIf simplify_eqs is set to true, simplify is called on f.\n\nAdditional keyworded arguments include name, which can be used to name the basis, and observed for defining observeables.\n\nFields\n\neqs\nThe equations of the basis\nstates\nDependent (state) variables\ncontrols\nControl variables\nps\nParameters\nobserved\nObserved\niv\nIndependent variable\nf\nInternal function representation of the basis\nname\nName of the basis\nsystems\nInternal systems\n\nExample\n\nusing ModelingToolkit\nusing DataDrivenDiffEq\n\n@parameters w[1:2] t\n@variables u[1:2](t)\n\nΨ = Basis([u; sin.(w.*u)], u, parameters = p, iv = t)\n\nNote\n\nThe keyword argument eval_expression controls the function creation behavior. eval_expression=true means that eval is used, so normal world-age behavior applies (i.e. the functions cannot be called from the function that generates them). If eval_expression=false, then construction via GeneralizedGenerated.jl is utilized to allow for same world-age evaluation. However, this can cause Julia to segfault on sufficiently large basis functions. By default eval_expression=false.\n\n\n\n\n\n","category":"type"},{"location":"basis/#Generators","page":"Basis","title":"Generators","text":"","category":"section"},{"location":"basis/","page":"Basis","title":"Basis","text":"monomial_basis\npolynomial_basis\nsin_basis\ncos_basis\nfourier_basis\nchebyshev_basis","category":"page"},{"location":"basis/#DataDrivenDiffEq.monomial_basis","page":"Basis","title":"DataDrivenDiffEq.monomial_basis","text":"monomial_basis(x)\nmonomial_basis(x, degree)\n\n\nConstructs an array containing monomial basis in the variables x up to degree c of the form [x₁, x₁^2, ... , x₁^c, x₂, x₂^2, ...].\n\n\n\n\n\n","category":"function"},{"location":"basis/#DataDrivenDiffEq.polynomial_basis","page":"Basis","title":"DataDrivenDiffEq.polynomial_basis","text":"polynomial_basis(x)\npolynomial_basis(x, degree)\n\n\nConstructs an array containing a polynomial basis in the variables x up to degree c of the form [x₁, x₂, x₃, ..., x₁^1 * x₂^(c-1)]. Mixed terms are included.\n\n\n\n\n\n","category":"function"},{"location":"basis/#DataDrivenDiffEq.sin_basis","page":"Basis","title":"DataDrivenDiffEq.sin_basis","text":"sin_basis(x, coefficients)\n\n\nConstructs an array containing a Sine basis in the variables x with coefficients c. If c is an Int returns all coefficients from 1 to c.\n\n\n\n\n\n","category":"function"},{"location":"basis/#DataDrivenDiffEq.cos_basis","page":"Basis","title":"DataDrivenDiffEq.cos_basis","text":"cos_basis(x, coefficients)\n\n\nConstructs an array containing a Cosine basis in the variables x with coefficients c. If c is an Int returns all coefficients from 1 to c.\n\n\n\n\n\n","category":"function"},{"location":"basis/#DataDrivenDiffEq.fourier_basis","page":"Basis","title":"DataDrivenDiffEq.fourier_basis","text":"fourier_basis(x, coefficients)\n\n\nConstructs an array containing a Fourier basis in the variables x with (integer) coefficients c. If c is an Int returns all coefficients from 1 to c.\n\n\n\n\n\n","category":"function"},{"location":"basis/#DataDrivenDiffEq.chebyshev_basis","page":"Basis","title":"DataDrivenDiffEq.chebyshev_basis","text":"chebyshev_basis(x, coefficients)\n\n\nConstructs an array containing a Chebyshev basis in the variables x with coefficients c. If c is an Int returns all coefficients from 1 to c.\n\n\n\n\n\n","category":"function"},{"location":"optimization/#[Sparse-Optimizers](@id_sparse_optimization)","page":"Sparse Optimization","title":"Sparse Optimizers","text":"","category":"section"},{"location":"optimization/","page":"Sparse Optimization","title":"Sparse Optimization","text":"STLSQ\nADMM\nSR3\nImplicitOptimizer\nADM","category":"page"},{"location":"optimization/#DataDrivenDiffEq.Optimize.STLSQ","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.STLSQ","text":"mutable struct STLSQ{T} <: DataDrivenDiffEq.Optimize.AbstractOptimizer{T}\n\nSTLQS is taken from the original paper on SINDY and implements a sequentially thresholded least squares iteration. λ is the threshold of the iteration. It is based upon this matlab implementation. It solves the following problem\n\nargmin_x frac12  Ax-b_2 + lambda x_2\n\nFields\n\nλ\nSparsity threshold\n\nExample\n\nopt = STLQS()\nopt = STLQS(1e-1)\nopt = STLQS(Float32[1e-2; 1e-1])\n\nNote\n\nThis was formally STRRidge and has been renamed.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#DataDrivenDiffEq.Optimize.ADMM","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.ADMM","text":"mutable struct ADMM{T, R} <: DataDrivenDiffEq.Optimize.AbstractOptimizer{T}\n\nADMM is an implementation of Lasso using the alternating direction methods of multipliers and loosely based on this implementation. It solves the following problem\n\nargmin_x frac12  Ax-b_2 + lambda x_1\n\nFields\n\nλ\nSparsity threshold\nρ\nAugmented Lagrangian parameter\n\nExample\n\nopt = ADMM()\nopt = ADMM(1e-1, 2.0)\n\n\n\n\n\n","category":"type"},{"location":"optimization/#DataDrivenDiffEq.Optimize.SR3","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.SR3","text":"mutable struct SR3{T, V, P<:DataDrivenDiffEq.Optimize.AbstractProximalOperator} <: DataDrivenDiffEq.Optimize.AbstractOptimizer{T}\n\nSR3 is an optimizer framework introduced by Zheng et. al., 2018 and used within Champion et. al., 2019. SR3 contains a sparsification parameter λ, a relaxation ν. It solves the following problem\n\nargmin_x w frac12  Ax-b_2 + lambda R(w) + fracnu2x-w_2\n\nWhere R is a proximal operator and the result is given by w.\n\nFields\n\nλ\nSparsity threshold\nν\nRelaxation parameter\nR\nProximal operator\n\nExample\n\nopt = SR3()\nopt = SR3(1e-2)\nopt = SR3(1e-3, 1.0)\nopt = SR3(1e-3, 1.0, SoftThreshold())\n\nNote\n\nOpposed to the original formulation, we use ν as a relaxation parameter, as given in Champion et. al., 2019. In the standard case of hard thresholding the sparsity is interpreted as λ = threshold^2 / 2, otherwise λ = threshold.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#DataDrivenDiffEq.Optimize.ImplicitOptimizer","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.ImplicitOptimizer","text":"mutable struct ImplicitOptimizer{T} <: DataDrivenDiffEq.Optimize.AbstractSubspaceOptimizer{T}\n\nOptimizer for finding a sparse implicit relationship via alternating the left hand side of the problem and solving the explicit problem, as introduced here.\n\nargmin_x x_0 stAx= 0\n\nFields\n\no\nExplicit Optimizer\n\nExample\n\nImplicitOptimizer(STLSQ())\nImplicitOptimizer(0.1f0, ADMM)\n\n\n\n\n\n","category":"type"},{"location":"optimization/#DataDrivenDiffEq.Optimize.ADM","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.ADM","text":"mutable struct ADM{T} <: DataDrivenDiffEq.Optimize.AbstractSubspaceOptimizer{T}\n\nOptimizer for finding a sparse basis vector in a subspace based on this paper. It solves the following problem\n\nargmin_x x_0 stAx= 0\n\nFields\n\nλ\nSparsity threshold\n\nExample\n\nADM()\nADM(λ = 0.1)\n\nNote\n\nWhile useable for implicit problems, a better choice in general is given by the ImplicitOptimizer which tends to be more robust.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#Related-Functions","page":"Sparse Optimization","title":"Related Functions","text":"","category":"section"},{"location":"optimization/","page":"Sparse Optimization","title":"Sparse Optimization","text":"sparse_regression!\nset_threshold!\nget_threshold","category":"page"},{"location":"optimization/#DataDrivenDiffEq.Optimize.sparse_regression!","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.sparse_regression!","text":"sparse_regression!(X, A, Y, opt; maxiter, abstol, progress, progress_outer, progress_offset, kwargs...)\n\n\nImplements a sparse regression, given an AbstractOptimizer or AbstractSubspaceOptimizer. X denotes the coefficient matrix, A the design matrix and Y the matrix of observed or target values. X can be derived via init(opt, A, Y). maxiter indicates the maximum iterations for each call of the optimizer, abstol the absolute tolerance of the difference between iterations in the 2 norm. If the optimizer is called with a Vector of thresholds, each maxiter indicates the maximum iterations for each threshold.\n\nIf progress is set to true, a progressbar will be available. progress_outer and progress_offset are used to compute the initial offset of the progressbar.\n\nIf used with a Vector of thresholds, the functions f with signature f(X, A, Y) and g with signature g(x, threshold) = G(f(X, A, Y)) with the arguments given as stated above can be passed in. These are used for finding the pareto-optimal solution to the sparse regression. \n\n\n\n\n\n","category":"function"},{"location":"optimization/#DataDrivenDiffEq.Optimize.set_threshold!","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.set_threshold!","text":"set_threshold!(opt, threshold)\n\n\nSet the threshold(s) of an optimizer to (a) specific value(s).\n\n\n\n\n\n","category":"function"},{"location":"optimization/#DataDrivenDiffEq.Optimize.get_threshold","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.get_threshold","text":"get_threshold(opt)\n\n\nGet the threshold(s) of an optimizer.\n\n\n\n\n\n","category":"function"},{"location":"optimization/#Proximal-Operators","page":"Sparse Optimization","title":"Proximal Operators","text":"","category":"section"},{"location":"optimization/","page":"Sparse Optimization","title":"Sparse Optimization","text":"SoftThreshold\nHardThreshold\nClippedAbsoluteDeviation","category":"page"},{"location":"optimization/#DataDrivenDiffEq.Optimize.SoftThreshold","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.SoftThreshold","text":"struct SoftThreshold <: DataDrivenDiffEq.Optimize.AbstractProximalOperator\n\nProximal operator which implements the soft thresholding operator.\n\nsign(x) * max(abs(x) - λ, 0)\n\nSee by Zheng et. al., 2018.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#DataDrivenDiffEq.Optimize.HardThreshold","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.HardThreshold","text":"struct HardThreshold <: DataDrivenDiffEq.Optimize.AbstractProximalOperator\n\nProximal operator which implements the hard thresholding operator.\n\nabs(x) > sqrt(2*λ) ? x : 0\n\nSee by Zheng et. al., 2018.\n\n\n\n\n\n","category":"type"},{"location":"optimization/#DataDrivenDiffEq.Optimize.ClippedAbsoluteDeviation","page":"Sparse Optimization","title":"DataDrivenDiffEq.Optimize.ClippedAbsoluteDeviation","text":"struct ClippedAbsoluteDeviation{T} <: DataDrivenDiffEq.Optimize.AbstractProximalOperator\n\nProximal operator which implements the (smoothly) clipped absolute deviation operator.\n\nabs(x) > ρ ? x : sign(x) * max(abs(x) - λ, 0)\n\nWhere ρ = 5λ per default.\n\n#Fields\n\nρ\nUpper threshold\n\nExample\n\nopt = ClippedAbsoluteDeviation()\nopt = ClippedAbsoluteDeviation(1e-1)\n\nSee by Zheng et. al., 2018.\n\n\n\n\n\n","category":"type"},{"location":"quickstart/#Quickstart","page":"Getting Started","title":"Quickstart","text":"","category":"section"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"In the following, we will use some of the techniques provided by DataDrivenDiffEq to infer some models.","category":"page"},{"location":"quickstart/#Linear-Systems-via-Dynamic-Mode-Decomposition","page":"Getting Started","title":"Linear Systems via Dynamic Mode Decomposition","text":"","category":"section"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"We will start by estimating the underlying dynamical system of a time discrete process based on some measurements via Dynamic Mode Decomposition. First, we model a simple linear system of the for u_i+1 = A u_i","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"using DataDrivenDiffEq\nusing LinearAlgebra\nusing ModelingToolkit\nusing OrdinaryDiffEq\nusing Plots # hide\n\nA = [0.9 -0.2; 0.0 0.2]\nu0 = [10.0; -10.0]\ntspan = (0.0, 11.0)\n\nf(u,p,t) = A*u\n\nsys = DiscreteProblem(f, u0, tspan)\nsol = solve(sys, FunctionMap())\nplot(sol) # hide\nsavefig(\"DMD_Example_1.png\") # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"(Image: )","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"To estimate the underlying operator in the states u_1 u_2, we simply define a discrete DataDrivenProblem using the measurements and time and solve the estimation problem using the DMDSVD algorithm for approximating the operator.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"\nprob = DiscreteDataDrivenProblem(sol)\n\nres = solve(prob, DMDSVD(), digits = 1)\nsystem = result(res)\nprintln(system) # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"The DataDrivenSolution contains an explicit result which is a Koopman, defining all necessary information, e.g. the associated operator (which corresponds to our abefore defined matrix A).","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Matrix(system)","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"In general, we can skip the expensive progress of deriving a callable symbolic system and return just the basic definitions using the operator_only keyword.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"res = solve(prob, DMDSVD(), digits = 1, operator_only = true)","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Where K is the associated operator given as its eigendecomposition, B is a possible mapping of inputs onto the states, C is the linear mapping from the lifted observeables back onto the original states and Q and P are used for updating the operator.","category":"page"},{"location":"quickstart/#Nonlinear-System-with-Extended-Dynamic-Mode-Decomposition","page":"Getting Started","title":"Nonlinear System with Extended Dynamic Mode Decomposition","text":"","category":"section"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Similarly, we can use the Extended Dynamic Mode Decomposition via a nonlinear Basis of observeables. Here, we look a rather famous example with a finite dimensional solution.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"using DataDrivenDiffEq\nusing LinearAlgebra\nusing ModelingToolkit\nusing OrdinaryDiffEq\nusing Plots\n\nfunction slow_manifold(du, u, p, t)\n    du[1] = p[1] * u[1]\n    du[2] = p[2] * (u[2] - u[1]^2)\nend\n\nu0 = [3.0; -2.0]\ntspan = (0.0, 5.0)\np = [-0.8; -0.7]\n\nproblem = ODEProblem(slow_manifold, u0, tspan, p)\nsolution = solve(problem, Tsit5(), saveat = 0.01)\nplot(solution) # hide\nsavefig(\"EDMD_Example_1.png\") # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"(Image: )","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Since we are dealing with an continuous system in time, we define the associated DataDrivenProblem accordingly using the measured states X, their derivates DX and the time t.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"prob = ContinuousDataDrivenProblem(solution)","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Additionally, we need to define the Basis for our lifting, before we solve the problem in the lifted space.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"@variables u[1:2]\nΨ = Basis([u; u[1]^2], u)\nres = solve(prob, Ψ, DMDPINV(), digits = 1)\nsystem = result(res)\nprintln(res) # hide\nprintln(system) # hide\nprintln(parameters(res)) # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"The underlying dynamics have been recovered correctly by the algorithm!","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"The eigendecomposition of the Koopman operator can be accessed via operator.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"operator(system)","category":"page"},{"location":"quickstart/#Nonlinear-Systems-Sparse-Identification-of-Nonlinear-Dynamics","page":"Getting Started","title":"Nonlinear Systems - Sparse Identification of Nonlinear Dynamics","text":"","category":"section"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"To find the underlying system without any Algortihms related to Koopman operator theory, we can use  Sparse Identification of Nonlinear Dynamics - SINDy for short. As the name suggests, it finds the sparsest basis of functions which build the observed trajectory. Again, we will start with a nonlinear system","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"using DataDrivenDiffEq\nusing LinearAlgebra\nusing ModelingToolkit\nusing OrdinaryDiffEq\nusing Plots\nusing Random\nusing Symbolics: scalarize\n\nRandom.seed!(1111) # Due to the noise\n\n# Create a nonlinear pendulum\nfunction pendulum(u, p, t)\n    x = u[2]\n    y = -9.81sin(u[1]) - 0.3u[2]^3 -3.0*cos(u[1]) - 10.0*exp(-((t-5.0)/5.0)^2)\n    return [x;y]\nend\n\nu0 = [0.99π; -1.0]\ntspan = (0.0, 15.0)\nprob = ODEProblem(pendulum, u0, tspan)\nsol = solve(prob, Tsit5(), saveat = 0.01)\n\n# Create the data with additional noise\nX = sol[:,:] + 0.1 .* randn(size(sol))\nDX = similar(sol[:,:])\n\nfor (i, xi) in enumerate(eachcol(sol[:,:]))\n    DX[:,i] = pendulum(xi, [], sol.t[i])\nend\n\nts = sol.t\nnothing #hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"To estimate the system, we first create a DataDrivenProblem via feeding in the measurement data. Using a Collocation method, it automatically provides the derivative. Control signals can be passed in as a function (u,p,t)->control or an array of measurements.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"prob = ContinuousDataDrivenProblem(X, ts, GaussianKernel() ,\n    U = (u,p,t)->[exp(-((t-5.0)/5.0)^2)], p = ones(2))\n\np1 = plot(ts, X', label = [\"Measurement\" nothing], color = :black, style = :dash, legend = :bottomleft, ylabel =\"Measurement\") # hide\nplot!(ts, prob.X', label = [\"Smoothed\" nothing], color = :red) # hide\np2 = plot(ts, prob.DX', label = nothing, color = :red, ylabel = \"Derivative\") # hide\nplot!(ts, DX', label = nothing, color = :black, style = :dash) # hide\np3 = plot(ts, prob.U', label = nothing, color = :red, xlabel = \"Time [s]\", ylabel = \"Control\") # hide\nplot(p1,p2,p3, layout = (3,1), size = (600,600)) # hide\nsavefig(\"SINDy_Example_Data.png\") # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"(Image: )","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Now we infer the system structure. First we define a Basis which collects all possible candidate terms. Since we want to use SINDy, we call solve with an Optimizer, in this case STLSQ which iterates different sparsity thresholds and returns a pareto optimal solution of the underlying sparse_regression!. Note that we include the control signal in the basis as an additional variable c.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"@variables u[1:2] c[1:1]\n@parameters w[1:2]\nu = scalarize(u)\nc = scalarize(c)\nw = scalarize(w)\n\nh = Num[sin.(w[1].*u[1]);cos.(w[2].*u[1]); polynomial_basis(u, 5); c]\n\nbasis = Basis(h, u, parameters = w, controls = c)\n\nλs = exp10.(-10:0.1:-1)\nopt = STLSQ(λs)\nres = solve(prob, basis, opt, progress = false, denoise = false, normalize = false, maxiter = 5000)\nprintln(res) # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"info: Info\nA more detailed description of the result can be printed via print(res, Val{true}), which also includes the discovered equations and parameter values.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Where the resulting DataDrivenSolution stores information about the infered model and the parameters:","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"system = result(res);\nparams = parameters(res);\nprintln(system) #hide\nprintln(params) #hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Since any system obtained via a solve command is a Basis and hence a subtype of an AbstractSystem defined in ModelingToolkit, we can simply simulate the result via:","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"infered_prob = ODEProblem(system, u0, tspan, parameters(res))\ninfered_solution = solve(infered_prob, Tsit5(), saveat = ts)\nplot(infered_solution, label = [\"Infered\" nothing], color = :red) # hide\n\nfunction pendulum(u, p, t) # hide\n    x = u[2] # hide\n    y = -9.81sin(u[1]) - 0.3u[2]^3 -3.0*cos(u[1]) # hide\n    return [x;y] # hide\nend # hide\n\nprob = ODEProblem(pendulum, u0, tspan) # hide\nsol = solve(prob, Tsit5(), saveat = 0.01) # hide\n\nplot!(sol, label = [\"Ground Truth\" nothing], color = :black, style = :dash) # hide\nsavefig(\"SINDy_Example_Data_Infered.png\") #hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"warning: Warning\nAs of now, the control input is dropped in the simulation of a system. We are working on this and pull requests are welcome!","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"(Image: )","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"As we can see above, the estimated system matches the ground truth reasonably well.","category":"page"},{"location":"quickstart/#Implicit-Nonlinear-Dynamics-:-Michaelis-Menten","page":"Getting Started","title":"Implicit Nonlinear Dynamics : Michaelis Menten","text":"","category":"section"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"But what if you want to estimate an implicitly defined system of the form f(u_t u p t) = 0? Do not worry, since there exists a solution : Implicit Sparse Identification. It has been originally described in this paper and currently there exist robust algorithms to identify these systems.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"We will focus on the Michaelis Menten Kinetics. As before, we will define the DataDrivenProblem and the Basis containing possible candidate functions for our sparse_regression!.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"using DataDrivenDiffEq\nusing LinearAlgebra\nusing ModelingToolkit\nusing Plots\nusing OrdinaryDiffEq\n\nfunction michaelis_menten(u, p, t)\n    [0.6 - 1.5u[1]/(0.3+u[1])]\nend\n\nu0 = [0.5]\n\nproblem_1 = ODEProblem(michaelis_menten, u0, (0.0, 4.0))\nsolution_1 = solve(problem_1, Tsit5(), saveat = 0.1)\nproblem_2 = ODEProblem(michaelis_menten, 2*u0, (4.0, 8.0))\nsolution_2 = solve(problem_2, Tsit5(), saveat = 0.1)\nX = [solution_1[:,:] solution_2[:,:]]\nts = [solution_1.t; solution_2.t]\n\nDX = similar(X)\nfor (i, xi) in enumerate(eachcol(X))\n    DX[:, i] = michaelis_menten(xi, [], ts[i])\nend\n\nprob = ContinuousDataDrivenProblem(X, ts, DX)\n\np1 = plot(ts, X', label = [\"Measurement\" nothing], color = :black, style = :dash, legend = :bottomleft, ylabel =\"Measurement\") # hide\np2 = plot(ts, DX', label = nothing, color = :black, style = :dash, ylabel = \"Derivative\", xlabel = \"Time [s]\") # hide\nplot(p1,p2, layout = (2,1), size = (600,400)) # hide\nsavefig(\"SINDy_Example_Data_2.png\") # hide\n\n@parameters t\nD = Differential(t)\n@variables u[1:1](t)\nh = [monomial_basis(u[1:1], 4)...]\nbasis = Basis([h; h .* D(u[1])], [u; D(u[1])], iv = t)\nprintln(basis) # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"(Image: )","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Next, we define the ImplicitOptimizer and solve the problem.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"opt = ImplicitOptimizer(4e-1)\nres = solve(prob, basis, opt, normalize = false, denoise = false, maxiter = 1000);\nprintln(res) # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"As we can see, the DataDrivenSolution already has good metrics. Inspection of the underlying system shows that the original equations have been recovered correctly:","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"system = result(res); # hide\nprintln(system)","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"warning: Warning\nRight now, Implicit results cannot be simulated without further processing in ModelingToolkit","category":"page"},{"location":"quickstart/#Implicit-Nonlinear-Dynamics-:-Cartpole","page":"Getting Started","title":"Implicit Nonlinear Dynamics : Cartpole","text":"","category":"section"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"The following is another example on how to use the ImplicitOptimizer and is taken from the original paper.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"As always, we start by creating a corresponding dataset.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"using DataDrivenDiffEq\nusing ModelingToolkit\nusing OrdinaryDiffEq\nusing LinearAlgebra\nusing Plots\ngr()\n\n\nfunction cart_pole(u, p, t)\n    du = similar(u)\n    F = -0.2 + 0.5*sin(6*t) # the input\n    du[1] = u[3]\n    du[2] = u[4]\n    du[3] = -(19.62*sin(u[1])+sin(u[1])*cos(u[1])*u[3]^2+F*cos(u[1]))/(2-cos(u[1])^2)\n    du[4] = -(sin(u[1])*u[3]^2 + 9.81*sin(u[1])*cos(u[1])+F)/(2-cos(u[1])^2)\n    return du\nend\n\nu0 = [0.3; 0; 1.0; 0]\ntspan = (0.0, 5.0)\ndt = 0.1\ncart_pole_prob = ODEProblem(cart_pole, u0, tspan)\nsolution = solve(cart_pole_prob, Tsit5(), saveat = dt)\n\nX = solution[:,:]\nDX = similar(X)\nfor (i, xi) in enumerate(eachcol(X))\n    DX[:, i] = cart_pole(xi, [], solution.t[i])\nend\nt = solution.t\n\nddprob = ContinuousDataDrivenProblem(\n    X , t, DX = DX[3:4, :], U = (u,p,t) -> [-0.2 + 0.5*sin(6*t)]\n)\n\n\nplot(solution) # hide\nsavefig(\"SINDy_Example_Data_3.png\") # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"(Image: )","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"Next, we define a sufficient Basis","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"using Symbolics: scalarize\n\n@variables u[1:4] du[1:2] x[1:1] t\n\n# Right now, we need to scalarize the array expression to combine them\nu = scalarize(u)\ndu = scalarize(du)\nx = scalarize(x)\n\npolys = polynomial_basis(u, 2)\npush!(polys, sin.(u[1]))\npush!(polys, cos.(u[1]))\npush!(polys, sin.(u[1])^2)\npush!(polys, cos.(u[1])^2)\npush!(polys, sin.(u[1]).*u[3:4]...)\npush!(polys, sin.(u[1]).*u[3:4].^2...)\npush!(polys, sin.(u[1]).*cos.(u[1])...)\npush!(polys, sin.(u[1]).*cos.(u[1]).*u[3:4]...)\npush!(polys, sin.(u[1]).*cos.(u[1]).*u[3:4].^2...)\nimplicits = [du;  du[1] .* u; du[2] .* u; du .* cos(u[1]);   du .* cos(u[1])^2; polys]\npush!(implicits, x...)\npush!(implicits, x[1]*cos(u[1]))\npush!(implicits, x[1]*sin(u[1]))\n\nbasis= Basis(implicits, [u; du], controls = x,  iv = t);\nprintln(basis) # hide","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"And solve the problem by varying over a sufficient set of thresholds for the associated optimizer. Additionally we activate the scale_coefficients option for the `ImplicitOptimizer, which helps to find sparse equations by normalizing the resulting coefficient matrix after each suboptimization.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"To evaluate the pareto optimal solution over, we use the functions f and g which can be passed as keyworded arguements into the solve function. f is a function with different signatures for different optimizers, but returns the L_0 norm of the coefficients and the L_2 error of the current model. g takes this vector and projects it down onto a scalar, using the L_2 norm per default. However, here we want to use the AIC  of the output of f. A noteworthy exception is of course, that we want only results with two or more active coefficents. Hence we modify g accordingly.","category":"page"},{"location":"quickstart/","page":"Getting Started","title":"Getting Started","text":"λ = [1e-4;5e-4;1e-3;2e-3;3e-3;4e-3;5e-3;6e-3;7e-3;8e-3;9e-3;1e-2;2e-2;3e-2;4e-2;5e-2;\n6e-2;7e-2;8e-2;9e-2;1e-1;2e-1;3e-1;4e-1;5e-1;6e-1;7e-1;8e-1;9e-1;1;1.5;2;2.5;3;3.5;4;4.5;5;\n6;7;8;9;10;20;30;40;50;100;200];\nopt = ImplicitOptimizer(λ)\n\n# Compute the AIC\ng(x) = x[1] <= 1 ? Inf : 2*x[1]-2*log(x[2])\nres = solve(ddprob, basis, opt, du, maxiter = 1000, g = g, scale_coefficients = true)\n\nprintln(res)\nprintln(result(res))\nprintln(parameter_map(res))","category":"page"},{"location":"contributions/#Contributions","page":"Contributing","title":"Contributions","text":"","category":"section"},{"location":"contributions/","page":"Contributing","title":"Contributing","text":"Contributions are welcome! To help, please:","category":"page"},{"location":"contributions/","page":"Contributing","title":"Contributing","text":"Open (or solve) an issue\nReview pull requests\nAdapt code to be more efficient\nWrite new optimizers or algorithms\nWrite tutorials or adapt the docs","category":"page"},{"location":"contributions/","page":"Contributing","title":"Contributing","text":"Feel free to write a private message to @AlCap23 for further discussion.","category":"page"},{"location":"prob_and_solve/#Problem-Definition-And-Solution","page":"Problems And Solution","title":"Problem Definition And Solution","text":"","category":"section"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"As can be seen from the introduction examples, DataDrivenDiffEq.jl tries to structurize the workflow in a similar fashion to other SciML packages by defining a DataDrivenProblem, dispatching on the solve command to return a DataDrivenSolution.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"A problem in the sense of identification, estimation or inference is defined by the data describing it. This data contains at least measurements of the states X, which would be sufficient to describe a DiscreteDataDrivenProblem with unit time steps similar to the first example on dynamic mode decomposition. Of course we can extend this to include time points t, control signals U or a function describing those u(x,p,t). Additionally, any parameters p known a priori can be included in the problem. In practice, this looks like","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"problem = DiscreteDataDrivenProblem(X)\nproblem = DiscreteDataDrivenProblem(X, t)\nproblem = DiscreteDataDrivenProblem(X, t, U)\nproblem = DiscreteDataDrivenProblem(X, t, U, p = p)\nproblem = DiscreteDataDrivenProblem(X, t, (x,p,t)->u(x,p,t))","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Similarly, a ContinuousDataDrivenProblem would need at least measurements and time-derivatives (X and DX) or measurements, time information and a way to derive the time derivatives(X, t and a Collocation method). Again, this can be extended by including a control input as measurements or a function and possible parameters.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"problem = ContinuousDataDrivenProblem(X, DX)\nproblem = ContinuousDataDrivenProblem(X, t, DX)\nproblem = ContinuousDataDrivenProblem(X, t, DX, U, p = p)\nproblem = ContinuousDataDrivenProblem(X, t, DX, (x,p,t)->u(x,p,t))\n# Using collocation\nproblem = ContinuousDataDrivenProblem(X, t, InterpolationMethod())\nproblem = ContinuousDataDrivenProblem(X, t, GaussianKernel())\nproblem = ContinuousDataDrivenProblem(X, t, U, InterpolationMethod())\nproblem = ContinuousDataDrivenProblem(X, t, U, GaussianKernel(), p = p)","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"You can also directly use a DESolution as an input to your DataDrivenProblem:","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"problem = DataDrivenProblem(sol; kwargs...)","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"which evaluates the function at the specific timepoints t using the parameters p of the original problem instead of using the interpolation. If you want to use the interpolated data, add the additional keyword use_interpolation = true.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"An additional type of problem is the DirectDataDrivenProblem, which does not assume any kind of causal relationship. It is defined by X and an observed output Y in addition to the usual arguments:","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"problem = DirectDataDrivenProblem(X, Y)\nproblem = DirectDataDrivenProblem(X, t, Y)\nproblem = DirectDataDrivenProblem(X, t, Y, U)\nproblem = DirectDataDrivenProblem(X, t, Y, p = p)\nproblem = DirectDataDrivenProblem(X, t, Y, (x,p,t)->u(x,p,t), p = p)","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Next up, we choose a method to solve the DataDrivenProblem. Depending on the input arguments and the type of problem, the function will return a result derived via Koopman or Sparse Optimization methods. Different options can be provided as well as a Basis used for lifting the measurements, to control different options like rounding, normalization or the progressbar depending on the inference method. Possible options are provided below.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"# Use a Koopman based inference\nres = solve(problem, DMDSVD(), kwargs...)\n# Use a sparse identification\nres = solve(problem, basis, STLQS(), kwargs...)","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"The DataDrivenSolution res contains a result which is the infered system and a Basis, metrics which is a NamedTuple containing different metrics like the L2 error of the infered system with the provided data and the AICC. These can be accessed via","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"# The infered system\nsystem = result(res)\n# The metrics\nm = metrics(res)\nm.Sparsity # No. of active terms / nonzero coefficients\nm.Error # L2 Error of all data\nm.Errors # Individual error of the different data rows\nm.AICC # AICC\nm.AICCs # ....","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Since the infered system is a parametrized equation, the corresponding parameters can be accessed and returned via","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"# Vector\nps = parameters(res)\n# Parameter map\nps = parameter_map(res)","category":"page"},{"location":"prob_and_solve/#optional_arguments","page":"Problems And Solution","title":"Optional Arguments","text":"","category":"section"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"info: Info\nThe keyword argument eval_expression controls the function creation behavior. eval_expression=true means that eval is used, so normal world-age behavior applies (i.e. the functions cannot be called from the function that generates them). If eval_expression=false, then construction via GeneralizedGenerated.jl is utilized to allow for same world-age evaluation. However, this can cause Julia to segfault on sufficiently large basis functions. By default eval_expression=false.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Koopman based algorithms can be called without a Basis, resulting in dynamic mode decomposition like methods, or with a basis for extened dynamic mode decomposition :","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"res = solve(problem, DMDSVD(), kwargs...)\nres = solve(problem, basis, DMDSVD(), kwargs...)","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"If control signals are present, they get processed according to this paper for dynamic mode decomposition and as described here for extended dynamic mode decomposition assuming a linear relationship on the operator.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Possible keyworded arguments include","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"B a linear mapping known a priori which maps the control signals onto the lifted states\ndigits controls the digits / rounding used for deriving the system equations (digits = 1 would round 10.02 to 10.0)\noperator_only returns a NamedTuple containing the operator, input and output mapping and matrices used for updating the operator as described here","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"info: Info\nIf eval_expression is set to true, the returning result of the Koopman based inference will not contain a parametrized equation, but rather use the numeric values of the operator/generator.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"SINDy based algorithms can be called like :","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"res = solve(problem, basis, STLQS(), kwargs...)\nres = solve(problem, basis, ImplicitOptimizer(), kwargs...)","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Where control signals are included in the candidate basis.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"For implicit optimizers, additionally an Vector{Num} of variables correspondng to u_t for implicitly defined equations f(u_t u p t) = 0 can be passed in","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"res = solve(problem, basis, ImplicitOptimizer(), implicits, kwargs...)","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"This indicates that we do not have coupling between two implicitly defined variables if not explicitly given. To elaborate this complicated sentence, consider the following:","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"basis = Basis([x, y, z], [x,y,z])\nres = solve(problem, basis, ImplicitOptimizer())","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Would allow solutions of the form x = y + z.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"basis = Basis([x, y, z], [x,y,z])\nres = solve(problem, basis, ImplicitOptimizer(), [x,y])","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Would exclude solutions of the form x = z or y = z since we declared x and y as implicit variables assuming they don't interact. However, defining","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"basis = Basis([x, y, z, x*y], [x,y,z])\nres = solve(problem, basis, ImplicitOptimizer(), [x,y])","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Would allow solutions of the form x = y*x + z or y = y*x + z to occur while suppressing x = y + z. This is due to the reason that y*x includes both x and y, so the function will get included in the evaluation.","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"Possible keyworded arguments include","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"normalize normalizes the data matrix Theta such that each row ( correspondng to candidate functions) has a 2-norm of 1.0\ndenoise applies optimal shrinking to the matrix Theta to remove the influence of noise\nmaxiter Maximum iterations of the used optimizer\nround rounds according to the currently used threshold of the optimizer","category":"page"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"warning: Warning\nFor additional keywords, have a look at the examples in the Quickstart section or into the source code (for now).","category":"page"},{"location":"prob_and_solve/#Types","page":"Problems And Solution","title":"Types","text":"","category":"section"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"DataDrivenProblem\nDataDrivenSolution","category":"page"},{"location":"prob_and_solve/#DataDrivenDiffEq.DataDrivenProblem","page":"Problems And Solution","title":"DataDrivenDiffEq.DataDrivenProblem","text":"struct DataDrivenProblem{dType, cType, probType} <: DataDrivenDiffEq.AbstractDataDrivenProblem{dType, cType, probType}\n\nThe DataDrivenProblem defines a general estimation problem given measurements, inputs and (in the near future) observations. Two construction methods are available:\n\nDiscreteDataDrivenProblem for time discrete systems\nContinousDataDrivenProblem for systems continouos in time\n\nboth are aliases for constructing a problem.\n\nFields\n\nX\nState measurements\nt\nTime measurements (optional)\nDX\nDifferental state measurements (optional); Used for time continuous problems\nY\nOutput measurements (optional); Used for direct problems\nU\nInput measurements (optional); Used for non-autonoumous problems\np\nParameters associated with the problem (optional)\n\nSignatures\n\nExample\n\nX, DX, t = data...\n\n# Define a discrete time problem\nprob = DiscreteDataDrivenProblem(X)\n\n# Define a continous time problem without explicit time points\nprob = ContinuousDataDrivenProblem(X, DX)\n\n# Define a continous time problem without explict derivatives\nprob = ContinuousDataDrivenProblem(X, t)\n\n# Define a discrete time problem with an input function as a function\ninput_signal(u,p,t) = t^2\nprob = DiscreteDataDrivenProblem(X, t, input_signal)\n\n\n\n\n\n","category":"type"},{"location":"prob_and_solve/#DataDrivenDiffEq.DataDrivenSolution","page":"Problems And Solution","title":"DataDrivenDiffEq.DataDrivenSolution","text":"struct DataDrivenSolution{R<:Union{Nothing, DataDrivenDiffEq.AbstractBasis}, S, P, A, O, M} <: DataDrivenDiffEq.AbstractDataDrivenSolution\n\nThe solution to a DataDrivenProblem derived via a certain algorithm. The solution is represented via an AbstractBasis, which makes it callable.\n\nFields\n\nres\nResult\nretcode\nReturncode\nps\nParameters used to derive the equations\nalg\nAlgorithm used for solution\nout\nOriginal Output\ninp\nOriginal Input\nmetrics\nError metrics\n\n\n\n\n\n","category":"type"},{"location":"prob_and_solve/#Functions","page":"Problems And Solution","title":"Functions","text":"","category":"section"},{"location":"prob_and_solve/","page":"Problems And Solution","title":"Problems And Solution","text":"result\nparameters\nparameter_map\nmetrics\noutput\nalgorithm\ninputs","category":"page"},{"location":"prob_and_solve/#DataDrivenDiffEq.result","page":"Problems And Solution","title":"DataDrivenDiffEq.result","text":"result(r)\n\n\nReturns the result of in form of an AbstractBasis.\n\n\n\n\n\n","category":"function"},{"location":"prob_and_solve/#ModelingToolkit.parameters","page":"Problems And Solution","title":"ModelingToolkit.parameters","text":"parameters(r)\n\n\nReturns the estimated parameters in form of an Vector.\n\n\n\n\n\n","category":"function"},{"location":"prob_and_solve/#DataDrivenDiffEq.parameter_map","page":"Problems And Solution","title":"DataDrivenDiffEq.parameter_map","text":"parameter_map(r)\n\n\nGenerate an mapping of the parameter values and symbolic representation useable to solve and ODESystem.\n\n\n\n\n\n","category":"function"},{"location":"prob_and_solve/#DataDrivenDiffEq.metrics","page":"Problems And Solution","title":"DataDrivenDiffEq.metrics","text":"metrics(r)\n\n\nReturns the metrics of the result in form of a NamedTuple.\n\n\n\n\n\n","category":"function"},{"location":"prob_and_solve/#DataDrivenDiffEq.output","page":"Problems And Solution","title":"DataDrivenDiffEq.output","text":"output(r)\n\n\nReturns the original output of the algorithm, e.g. an AbstractArray of coefficients for sparse regression.\n\n\n\n\n\n","category":"function"},{"location":"prob_and_solve/#DataDrivenDiffEq.algorithm","page":"Problems And Solution","title":"DataDrivenDiffEq.algorithm","text":"algorithm(r)\n\n\nReturns the algorithm used to derive the solution.\n\n\n\n\n\n","category":"function"},{"location":"prob_and_solve/#DataDrivenDiffEq.inputs","page":"Problems And Solution","title":"DataDrivenDiffEq.inputs","text":"inputs(r)\n\n\nReturns the original inputs, most commonly the DataDrivenProblem and the Basis used to derive the solution.\n\n\n\n\n\n","category":"function"},{"location":"#DataDrivenDiffEq.jl","page":"Home","title":"DataDrivenDiffEq.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DataDrivenDiffEq.jl is a package for estimating equation-free and equation-based models for discrete and continuous differential equations.","category":"page"},{"location":"","page":"Home","title":"Home","text":"As opposed to parameter identification, these methods aim to find the governing equations of motion automatically from a given set of data. They do not require a known model as input. Instead, these methods take in data and return the differential equation model which generated the data.","category":"page"},{"location":"","page":"Home","title":"Home","text":"There are various avenues in which structural estimation can occur. However, the main branches are: do you want to know the equations in a human-understandable manner, or is it sufficient to have a function that predicts the derivative and generates the correct time series? We will refer to methods which return symbolic forms of the differential equation as structural identification, while those which return functions only for prediction as structural estimation.","category":"page"},{"location":"#Package-Overview","page":"Home","title":"Package Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"DataDrivenDiffEq.jl currently implements the following algorithms for structural estimation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Dynamic Mode Decomposition\nExtended Dynamic Mode Decomposition\nSparse Identification of Nonlinear Dynamics\nImplicit Sparse Identification of Nonlinear Dynamics","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To use DataDrivenDiffEq.jl, simply install it via:","category":"page"},{"location":"","page":"Home","title":"Home","text":"]add DataDrivenDiffEq\nusing DataDrivenDiffEq","category":"page"},{"location":"koopman/#Koopman","page":"Koopman","title":"Koopman","text":"","category":"section"},{"location":"koopman/","page":"Koopman","title":"Koopman","text":"Koopman","category":"page"},{"location":"koopman/#DataDrivenDiffEq.Koopman","page":"Koopman","title":"DataDrivenDiffEq.Koopman","text":"mutable struct Koopman{O, M, G, T} <: DataDrivenDiffEq.AbstractKoopman\n\nA special basis over the states with parameters , independent variable  and possible exogenous controls. It extends an AbstractBasis, which also stores information about the lifted dynamics, specified by a sufficient matrix factorization, an output mapping and internal variables to update the equations. It can be called with the typical SciML signature, meaning out of place with f(u,p,t) or in place with f(du, u, p, t). If control inputs are present, it is assumed that no control corresponds to zero for all inputs. The corresponding function calls are f(u,p,t,inputs) and f(du,u,p,t,inputs) and need to be specified fully.\n\nIf linear_independent is set to true, a linear independent basis is created from all atom function in f.\n\nIf simplify_eqs is set to true, simplify is called on f.\n\nAdditional keyworded arguments include name, which can be used to name the basis, and observed for defining observeables.\n\nFields\n\neqs\nThe equations of the basis\nstates\nDependent (state) variables\ncontrols\nControl variables\nps\nParameters\nobserved\nObserved\niv\nIndependent variable\nf\nInternal function representation of the basis\nlift\nAssociated lifting of the operator\nname\nName of the basis\nsystems\nInternal systems\nis_discrete\nDiscrete or time continuous\nK\nThe operator/generator of the dynamics\nC\nMapping back onto the observed states\nQ\nInternal matrix Q used for updating\nP\nInternal matrix P used for updating\n\nNote\n\nThe keyword argument eval_expression controls the function creation behavior. eval_expression=true means that eval is used, so normal world-age behavior applies (i.e. the functions cannot be called from the function that generates them). If eval_expression=false, then construction via GeneralizedGenerated.jl is utilized to allow for same world-age evaluation. However, this can cause Julia to segfault on sufficiently large basis functions. By default eval_expression=false.\n\n\n\n\n\n","category":"type"},{"location":"koopman/#Functions","page":"Koopman","title":"Functions","text":"","category":"section"},{"location":"koopman/","page":"Koopman","title":"Koopman","text":"is_discrete\nis_continuous\nDataDrivenDiffEq.eigen\nDataDrivenDiffEq.eigvals\nDataDrivenDiffEq.eigvecs\nmodes\nfrequencies\noperator\ngenerator\nupdatable\nis_stable\nupdate!","category":"page"},{"location":"koopman/#DataDrivenDiffEq.is_discrete","page":"Koopman","title":"DataDrivenDiffEq.is_discrete","text":"is_discrete(k)\n\n\nReturns if the AbstractKoopmanOperator k is discrete in time.\n\n\n\n\n\nis_discrete(_)\n\n\nCheck if the problem is time discrete.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#DataDrivenDiffEq.is_continuous","page":"Koopman","title":"DataDrivenDiffEq.is_continuous","text":"is_continuous(k)\n\n\nReturns if the AbstractKoopmanOperator k is continuous in time.\n\n\n\n\n\nCheck if the problem is time continuous.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#LinearAlgebra.eigen","page":"Koopman","title":"LinearAlgebra.eigen","text":"eigen(k)\n\n\nReturn the eigendecomposition of the AbstractKoopmanOperator.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#LinearAlgebra.eigvals","page":"Koopman","title":"LinearAlgebra.eigvals","text":"eigvals(k)\n\n\nReturn the eigenvalues of the AbstractKoopmanOperator.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#LinearAlgebra.eigvecs","page":"Koopman","title":"LinearAlgebra.eigvecs","text":"eigvecs(k)\n\n\nReturn the eigenvectors of the AbstractKoopmanOperator.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#DataDrivenDiffEq.modes","page":"Koopman","title":"DataDrivenDiffEq.modes","text":"modes(k)\n\n\nReturn the eigenvectors of a continuous AbstractKoopmanOperator.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#DataDrivenDiffEq.frequencies","page":"Koopman","title":"DataDrivenDiffEq.frequencies","text":"frequencies(k)\n\n\nReturn the eigenvalues of a continuous AbstractKoopmanOperator.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#DataDrivenDiffEq.operator","page":"Koopman","title":"DataDrivenDiffEq.operator","text":"operator(k)\n\n\nReturn the approximation of the discrete Koopman operator stored in k.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#DataDrivenDiffEq.generator","page":"Koopman","title":"DataDrivenDiffEq.generator","text":"generator(k)\n\n\nReturn the approximation of the continuous Koopman generator stored in k.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#DataDrivenDiffEq.updatable","page":"Koopman","title":"DataDrivenDiffEq.updatable","text":"updatable(k)\n\n\nReturns true if the AbstractKoopmanOperator is updatable.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#DataDrivenDiffEq.is_stable","page":"Koopman","title":"DataDrivenDiffEq.is_stable","text":"is_stable(k)\n\n\nReturns true if either:\n\nthe Koopman operator has just eigenvalues with magnitude less than one or\nthe Koopman generator has just eigenvalues with a negative real part\n\n\n\n\n\n","category":"function"},{"location":"koopman/#DataDrivenDiffEq.update!","page":"Koopman","title":"DataDrivenDiffEq.update!","text":"update!(k, X, Y; p, t, U, threshold)\n\n\nUpdate the Koopman k given new data X and Y. The operator is updated in place if the L2 error of the prediction exceeds the threshold.\n\np and t are the parameters of the basis and the vector of timepoints, if necessary.\n\n\n\n\n\n","category":"function"},{"location":"koopman/#koopman_algorithms","page":"Koopman","title":"Algortihms","text":"","category":"section"},{"location":"koopman/","page":"Koopman","title":"Koopman","text":"DMDPINV\nDMDSVD\nTOTALDMD","category":"page"},{"location":"koopman/#DataDrivenDiffEq.DMDPINV","page":"Koopman","title":"DataDrivenDiffEq.DMDPINV","text":"mutable struct DMDPINV <: DataDrivenDiffEq.AbstractKoopmanAlgorithm\n\nApproximates the Koopman operator K based on\n\nK = Y / X\n\nwhere Y and X are data matrices. Returns a  Eigen factorization of the operator.\n\nFields\n\nSignatures\n\n\n\n\n\n","category":"type"},{"location":"koopman/#DataDrivenDiffEq.DMDSVD","page":"Koopman","title":"DataDrivenDiffEq.DMDSVD","text":"mutable struct DMDSVD{T} <: DataDrivenDiffEq.AbstractKoopmanAlgorithm\n\nApproximates the Koopman operator K based on the singular value decomposition of X such that:\n\nK = Y*V*Σ*U'\n\nwhere Y and X = U*Σ*V' are data matrices. The singular value decomposition is truncated via the truncation parameter, which can either be an Int indiciating an index based truncation or a Real indiciating a tolerance based truncation. Returns a Eigen factorization of the operator.\n\nFields\n\ntruncation\nIndiciates the truncation\n\nSignatures\n\n\n\n\n\n","category":"type"},{"location":"koopman/#DataDrivenDiffEq.TOTALDMD","page":"Koopman","title":"DataDrivenDiffEq.TOTALDMD","text":"mutable struct TOTALDMD{R, A} <: DataDrivenDiffEq.AbstractKoopmanAlgorithm\n\nApproximates the Koopman operator K with the algorithm alg over the rank-reduced data matrices Xᵣ = X Qᵣ and Yᵣ = Y Qᵣ, where Qᵣ originates from the singular value decomposition of the joint data Z = [X; Y]. Based on this paper.\n\nIf rtol ∈ (0, 1) is given, the singular value decomposition is reduced to include only entries bigger than rtol*maximum(Σ). If rtol is an integer, the reduced SVD up to rtol is used for computation.\n\nFields\n\ntruncation\nalg\n\nSignatures\n\n\n\n\n\n","category":"type"},{"location":"symbolic_regression/#Symbolic-Regression","page":"Symbolic Regression","title":"Symbolic Regression","text":"","category":"section"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Using sparse regression limits the discovery to a generalized linear model where it is assumed that the nonlinear basis can capture the underlying function properly. Another approach is to use a general expression tree, which commonly encodes the function to discover as a binary tree where the nodes represent unary or binary operators acting on their children. DataDrivenDiffEq includes the following symbolic regression algorithms.","category":"page"},{"location":"symbolic_regression/#SymbolicRegression","page":"Symbolic Regression","title":"SymbolicRegression","text":"","category":"section"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"warning: Warning\nThis feature requires the explicit loading of SymbolicRegression.jl in addition to DataDrivenDiffEq. It will only be useable if used likeusing DataDrivenDiffEq\nusing SymbolicRegression","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"DataDrivenDiffEq provides an interface to SymbolicRegression.jl to solve a DataDrivenProblem:","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"using DataDrivenDiffEq\nusing LinearAlgebra\nusing Random\nusing SymbolicRegression\n\n\nRandom.seed!(1223)\n# Generate a multivariate function for SymbolicRegression\nX = rand(2,20)\nf(x) = [sin(x[1]); exp(x[2])]\nY = hcat(map(f, eachcol(X))...)\n\n# Define the options\nopts = EQSearch([+, *, sin, exp], maxdepth = 1, progress = false, multithreading = false)\n\n# Define the problem\nprob = DirectDataDrivenProblem(X, Y)\n\n# Solve the problem\nres = solve(prob, opts)\nsys = result(res)\nprintln(sys) #hide","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Where solve is used with EQSearch, which wraps Options provided by SymbolicRegression.jl. Additional keyworded arguments are max_iter = 10, which defines the number of iterations, weights which weight the measurements of the dependent variable (e.g. X, DX or Y depending on the DataDrivenProblem), numprocs which indicates the number of processes to use, procs for use with manually setup processes, multithreading = false for multithreading and runtests = true which performs initial testing on the environment to check for possible errors. It mimics the behaviour of EquationSearch.","category":"page"},{"location":"symbolic_regression/#Related-Types","page":"Symbolic Regression","title":"Related Types","text":"","category":"section"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"EQSearch","category":"page"},{"location":"symbolic_regression/#DataDrivenDiffEq.EQSearch","page":"Symbolic Regression","title":"DataDrivenDiffEq.EQSearch","text":"struct EQSearch <: DataDrivenDiffEq.AbstractSymbolicRegression\n\nOptions for using SymbolicRegression.jl within the solve function. Automatically creates Options with the given specification. Sorts the operators stored in functions into unary and binary operators on conversion.\n\nFields\n\nfunctions\nOperators used for symbolic regression\nkwargs\nAdditionally keyworded arguments passed to SymbolicRegression.Options\n\n\n\n\n\n","category":"type"},{"location":"symbolic_regression/#OccamNet","page":"Symbolic Regression","title":"OccamNet","text":"","category":"section"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"warning: Warning\nThis feature requires the explicit loading of Flux.jl in addition to DataDrivenDiffEq. It will only be useable if used likeusing DataDrivenDiffEq\nusing Flux","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"As introduced in Interpretable Neuroevolutionary Models for Learning Non-Differentiable Functions and Programs , OccamNet is a special form of symbolic regression which uses a probabilistic approach to equation discovery by using a feedforward multilayer neural network. In constrats to normal architectures, each layers weight reflect the probability which inputs to use. Additionally not only a single activation function is used, but a set of functions. Similar to simulated annealing, a temperature is included to control the exploration of possible functions.","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"DataDrivenDiffEq offers two main interfaces to OccamNet: a Flux based API with Flux.train! and a solve(...) function.","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Consider the following example, where we want to discover a vector valued function","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"using DataDrivenDiffEq\nusing LinearAlgebra\nusing ModelingToolkit\nusing Flux\nusing Random\n\n# Due to random values\nRandom.seed!(1223)\n\n# Generate a multivariate dataset\nX = rand(2,10)\nf(x) = [sin(π*x[2]+x[1]); exp(x[2])]\nY = hcat(map(f, eachcol(X))...)","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Next, we define our network","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"net = OccamNet(2, 2, 3, Function[sin, +, *, exp], skip = true, constants = Float64[π])","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Where 2,2,3 refers to input and output dimension and the number of layers without the output layer. We also define that each layer uses the functions sin, +, *, exp as activations an use a π as a constant, which get concanated to the input data. Additionally, skip indicates the useage of skip connections, which allow the output of each layer to be passed onto the output layer directly.","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"To train the network over 100 epochs using ADAM, we type","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Flux.train!(net, X, Y, ADAM(1e-2), 100, routes = 100, nbest = 3)","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Under the hood, we select routes possible routes through the network based on the probability reflect by the ProbabilityLayer forming the network. From these we take the nbest candidates to train the parameters of the network, meaning increase the probability of those routes.","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Lets have a look at some possible equations after the initial training. We can use rand to sample a route through the network, compute the output probability with probability and transform it into analytical equations by simply using ModelingToolkits variables as input. The call net(x, route) uses the route to compute just the element on this path.","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"@variables x[1:2]\n\nfor i in 1:10\n  route = rand(net)\n  prob = probability(net, route)\n  eq = simplify.(net(x, route))\n  print(eq , \" with probability \",  prob, \"\\n\")\nend","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"We see the networks proposals are not very certain. Hence, we will train for some more epochs and look at the output again.","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Flux.train!(net, X, Y, ADAM(1e-2), 900, routes = 100, nbest = 3)\n\nfor i in 1:10\n  route = rand(net)\n  prob = probability(net, route)\n  eq = simplify.(net(x, route))\n  print(eq , \" with probability \",  prob, \"\\n\")\nend","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"The network is quite certain about the equation now, which is in fact our unknown mapping. To extract the solution with the highest probability, we set the temperature of the underlying distribution to a very low value. In the limit of t ↦ 0 we approach a dirac distribution for the and hence extracting the most likely terms.","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"set_temp!(net, 0.01)\nroute = rand(net)\nprob = probability(net, route)\neq = simplify.(net(x, route))\nprint(eq , \" with probability \",  prob, \"\\n\")","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"The same procedure is automated in the solve function. Using the same data, we wrap the algorithms information in the OccamSR struct and define a DataDrivenProblem:","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"# Define the problem\nddprob = DirectDataDrivenProblem(X, Y)\n# Define the algorithm\nsr_alg = OccamSR(functions = Function[sin, +, *, exp], skip = true, layers = 3, constants = [π])\n# Solve the problem\nres = solve(ddprob, sr_alg, ADAM(1e-2), max_iter = 1000, routes = 100, nbest = 3)\nprintln(res) #hide","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"Within solve the network is generated using the information provided by the DataDrivenProblem in form of states, control and independent variables as well as the specified options, followed by training the network and extracting the equation with the highest probability by setting the temperature as above. After computing additional metrics, a DataDrivenSolution is returned where the equations are transformed  into a Basis useable with ModelingToolkit.","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"The metrics can be accessed via","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"metrics(res)","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"and the resulting Basis by","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"result(res)\nprintln(result(res)) #hide","category":"page"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"info: Info\nRight now, the resulting basis is not using parameters, but raw numerical values.","category":"page"},{"location":"symbolic_regression/#Related-Types-2","page":"Symbolic Regression","title":"Related Types","text":"","category":"section"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"OccamNet\nOccamSR\nProbabilityLayer","category":"page"},{"location":"symbolic_regression/#DataDrivenDiffEq.OccamNet","page":"Symbolic Regression","title":"DataDrivenDiffEq.OccamNet","text":"mutable struct OccamNet{F, C, P} <: DataDrivenDiffEq.AbstractOccam\n\nDefines a OccamNet which learns symbolic expressions from data using a probabalistic approach. See Interpretable Neuroevolutionary Models for Learning Non-Differentiable Functions and Programs  for more details.\n\nIt get constructed via\n\nnet = OccamNet(inp::Int, outp::Int, layers::Int, f::Vector{Function}, t::Real = 1.0; constants = typeof(t)[], parameters::Int = 0, skip::Bool = false, init_w = ones, init_p = Flux.glorot_uniform)\n\ninp describes the size of the input domain, outp the size of the output domain, layers the number of layers (including the input layer and excluding the linear output layer) and f the functions to be used. Optional is the temperature t which is set to 1.0 at the beginning.\n\nKeyworded arguments are constants, a vector of constants like π, ℯ which can concanated to the input, the number of trainable parameters and if skip connections should be used. The constructors to the weights and parameters can be passed in via init_w and init_p.\n\nOccamNet is callable with and without a specific route, which can be sampled from the networks weights via rand(net).\n\nFields\n\nc\nThe Chain representing the network\nconstants\nAdditional constants added to the input which are not trainable.\nparameters\nAdditional parameters added to the input which are trainable.\n\n\n\n\n\n","category":"type"},{"location":"symbolic_regression/#DataDrivenDiffEq.OccamSR","page":"Symbolic Regression","title":"DataDrivenDiffEq.OccamSR","text":"struct OccamSR{F, C, T} <: DataDrivenDiffEq.AbstractSymbolicRegression\n\nOptions for using OccamNet within the solve function. Automatically creates a network with the given specification.\n\nFields\n\nfunctions\nFunctions used within the network\nconstants\nConstants added to the input\nlayers\nNumber of layers\nparameters\nNumber of parameters\nskip\nActivate skip connections\n\n\n\n\n\n","category":"type"},{"location":"symbolic_regression/#DataDrivenDiffEq.ProbabilityLayer","page":"Symbolic Regression","title":"DataDrivenDiffEq.ProbabilityLayer","text":"mutable struct ProbabilityLayer{F, W, T, A} <: DataDrivenDiffEq.AbstractProbabilityLayer\n\nDefines a basic ProbabilityLayer in which the parameters act as probabilities via the softmax function for an array of functions.\n\nThe layer is callable either via layer(x), using all weights to form the output or by layer(x, route) where route is the result of rand(layer) which samples the function arguments from the underlying distribution.\n\nFields\n\nop\nNonlinear functions forming the basis of the layer\nweight\nWeights\nt\nTemperature controlling the shape of the distribution\narieties\nArieties of the functions\nskip\nSkip connection\n\n\n\n\n\n","category":"type"},{"location":"symbolic_regression/#Related-Functions","page":"Symbolic Regression","title":"Related Functions","text":"","category":"section"},{"location":"symbolic_regression/","page":"Symbolic Regression","title":"Symbolic Regression","text":"set_temp!\nprobability\nlogprobability\nprobabilities\nlogprobabilities","category":"page"},{"location":"symbolic_regression/#DataDrivenDiffEq.set_temp!","page":"Symbolic Regression","title":"DataDrivenDiffEq.set_temp!","text":"set_temp!(p, t)\n\n\nSet the temperature of the ProbabilityLayer or OccamNet.\n\n\n\n\n\n","category":"function"},{"location":"symbolic_regression/#DataDrivenDiffEq.probability","page":"Symbolic Regression","title":"DataDrivenDiffEq.probability","text":"probability(o, route)\n\n\nReturns the probability of the result of the OccamNet using the specific route.\n\n\n\n\n\n","category":"function"},{"location":"symbolic_regression/#DataDrivenDiffEq.logprobability","page":"Symbolic Regression","title":"DataDrivenDiffEq.logprobability","text":"logprobability(o, route)\n\n\nReturns the logprobability of the result of the OccamNet using the specific route.\n\n\n\n\n\n","category":"function"},{"location":"symbolic_regression/#DataDrivenDiffEq.probabilities","page":"Symbolic Regression","title":"DataDrivenDiffEq.probabilities","text":"probabilities(p)\n\n\nReturn the probability associated with the ProbabilityLayer or OccamNet by applying softmax on the weights.\n\n\n\n\n\n","category":"function"},{"location":"symbolic_regression/#DataDrivenDiffEq.logprobabilities","page":"Symbolic Regression","title":"DataDrivenDiffEq.logprobabilities","text":"logprobabilities(p)\n\n\nReturn the logprobability associated with the ProbabilityLayer or OccamNet by applying logsoftmax on the weights.\n\n\n\n\n\n","category":"function"}]
}
